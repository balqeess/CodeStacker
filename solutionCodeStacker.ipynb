{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2a78f-368a-4967-a37c-145dbe18b79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c11b0b-59e7-4d0e-80b3-e2b3b238be2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"denoising-shabby-pages\"\n",
    "train_path = os.path.join(base_path, \"train\")\n",
    "validate_path = os.path.join(base_path, \"validate\")\n",
    "test_path = os.path.join(base_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb822b-f2cc-448e-b5a4-15d8ee14f5f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# creates a dateframe for train, test and validate sets \n",
    "# within each df there are three columns indcies, cleaned paths image, shabby paths image \n",
    "# and lets the computer know that these images are pairs\n",
    "def create_dataset_dataframe(image_dir):\n",
    "    shabby_dir = os.path.join(image_dir, \"shabby\")\n",
    "    cleaned_dir = os.path.join(image_dir, \"cleaned\")\n",
    "\n",
    "    shabby_filenames = os.listdir(shabby_dir)\n",
    "    cleaned_filenames = os.listdir(cleaned_dir)\n",
    "\n",
    "    dataset_list = []\n",
    "\n",
    "    for idx, (shabby_filename, cleaned_filename) in enumerate(zip(shabby_filenames, cleaned_filenames)):\n",
    "        dataset_list.append({\n",
    "            \"shabby_image_path\": os.path.join(shabby_dir, shabby_filename),\n",
    "            \"cleaned_image_path\": os.path.join(cleaned_dir, cleaned_filename),\n",
    "            \n",
    "        })\n",
    "\n",
    "    dataset_df = pd.DataFrame(dataset_list)\n",
    "    return dataset_df\n",
    "\n",
    "train_df = create_dataset_dataframe(train_path)\n",
    "validate_df = create_dataset_dataframe(validate_path)\n",
    "test_df = create_dataset_dataframe(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e5065-b659-4a17-9108-a7425292a448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500c3e3-97f5-4684-8f47-2f5b10c77392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9485d28-4e6e-443c-88d1-267e24565421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  prints out the size of each image\n",
    "import cv2\n",
    "size=[]\n",
    "for i in range(len(test_df)):\n",
    "  img_gt = cv2.imread(test_df['shabby_image_path'].iloc[i])\n",
    "  size.append(img_gt.shape)\n",
    "\n",
    "test_df['image size'] = size\n",
    "test_df['image size'] = test_df['image size'].astype(str)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ed44f-d7fe-4c99-aff7-c4a241f68b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (100, 10))\n",
    "y = list(test_df['image size'].value_counts())\n",
    "x = test_df['image size'].value_counts().index.tolist()\n",
    "plt.bar(x,y)\n",
    "plt.title(\"Images vs Size\")\n",
    "plt.xlabel(\"Size of images\")\n",
    "plt.ylabel(\"No. of images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a64c2f-0010-4fa5-a242-8066afa83a19",
   "metadata": {},
   "source": [
    "all of the images have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42964607-26c2-4dbc-a03c-e7b9832145f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prints three samples of pairs of cleaned and shabby images\n",
    "sample = train_df.sample(3)\n",
    "fig, ax = plt.subplots(len(sample),2,figsize=(30,30))\n",
    "for i in range(len(sample)):\n",
    "  img = cv2.imread(sample['cleaned_image_path'].iloc[i])\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  # img = cv2.resize(img,(512,512))\n",
    "  ax[i][0].imshow(img)\n",
    "  ax[i][0].get_xaxis().set_visible(False)\n",
    "  ax[i][0].get_yaxis().set_visible(False)\n",
    "  ax[i][0].title.set_text(\"Cleaned Image\")\n",
    "  \n",
    "  img = cv2.imread(sample['shabby_image_path'].iloc[i])\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  # img = cv2.resize(img,(512,512))\n",
    "  ax[i][1].imshow(img)\n",
    "  ax[i][1].get_xaxis().set_visible(False)\n",
    "  ax[i][1].get_yaxis().set_visible(False)\n",
    "  ax[i][1].title.set_text(\"Shabby Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa9801-c551-4254-be57-19d2d27995bb",
   "metadata": {},
   "source": [
    "Observation : One can see, there is significant amount of noise in the nosiy images and the ground truth images shows the corresponding clean images free from noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81e122-62c1-4a32-8d84-b89664e1e08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from patchify import patchify, unpatchify\n",
    "# func to create patches \n",
    "def patches(img,patch_size):\n",
    "  patches = patchify(img, (patch_size, patch_size, 3), step=patch_size)\n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467a44d-e0ba-4381-b15a-fd5890714427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = pd.DataFrame({'cleaned_image_path':['a','denoising-shabby-pages/train/cleaned/0001-USPS-dmm300_608.pdf-15.png'], 'shabby_image_path':['b','denoising-shabby-pages/train/shabby/0001-USPS-dmm300_608.pdf-15.png']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312b528-0a58-45bf-ab22-3160b83d6280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3d202-7aab-485c-a2f1-18129f9dab01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating patches for a Ground Truth Image of the specified sample\n",
    "path = sample['cleaned_image_path'].iloc[1]\n",
    "img = cv2.imread(path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print('Image shape: {}'.format(img.shape))\n",
    "\n",
    "patches_gt = patches(img,100)\n",
    "print('Patch shape: {}'.format(patches_gt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09526daf-6788-412b-9c3a-b9c3556d1790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating patches for a Noisy Image\n",
    "path = sample['shabby_image_path'].iloc[1]\n",
    "img = cv2.imread(path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print('Image shape: {}'.format(img.shape))\n",
    "\n",
    "patches_nsy = patches(img,100)\n",
    "print('Patch shape: {}'.format(patches_nsy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03540235-1b10-48aa-8168-ebcb9a073d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = patches_nsy.shape[0]\n",
    "cols = patches_nsy.shape[1]\n",
    "fig, axs = plt.subplots(rows,cols,figsize=(20,10))\n",
    "for i in range(rows):\n",
    "  for j in range(cols):\n",
    "    axs[i][j].imshow(patches_nsy[i][j][0])\n",
    "    axs[i][j].get_xaxis().set_visible(False)\n",
    "    axs[i][j].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b242eb-fb71-4d57-8167-2fdc46cc9f85",
   "metadata": {},
   "source": [
    "This is what patches does. It splits the images into different patches based on a given patch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4687bf7-f9c4-4f95-98e0-4222e525500a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# further split the dataframe into shabby and cleaned for X, y respectively\n",
    "X_train_dataframe = train_df[[\"shabby_image_path\"]]\n",
    "y_train_dataframe = train_df[[\"cleaned_image_path\"]]\n",
    "\n",
    "X_validate_dataframe = validate_df[[\"shabby_image_path\"]]\n",
    "y_validate_dataframe = validate_df[[\"cleaned_image_path\"]]\n",
    "\n",
    "X_test_dataframe = test_df[[\"shabby_image_path\"]]\n",
    "y_test_dataframe = test_df[[\"cleaned_image_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7937b4-4019-445e-8600-d84205854cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87c053-39f0-4b4f-8d3a-eaa3ca6b93d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51263797-c218-478d-bb03-79be765b68b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7abaa9-8553-4eb6-9055-88b925264e27",
   "metadata": {},
   "source": [
    "We will be creating patches for all the images. Experiments have shown that splitting images into patches and using these patches for training improve model performance in denoising.\n",
    "In regard to that, we will resize all the images to a fixed size of 1024 x 1024 and create patches with patch size 256 x 256.\n",
    "The patches are extracted from resized images and will be used for testing a denoising model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc3e65-a7e2-4f50-81cc-a7136b99cb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating patches for X_train and y_train\n",
    "X_test_patches = []\n",
    "y_test_patches = []\n",
    "for i in range(len(X_test_dataframe)):\n",
    "  path = X_test_dataframe.at[i, \"shabby_image_path\"]\n",
    "  img_nsy = cv2.imread(path)\n",
    "  img_nsy = cv2.cvtColor(img_nsy, cv2.COLOR_BGR2RGB)\n",
    "  img_nsy = cv2.resize(img_nsy,(1024,1024))  #resizing the X_test images\n",
    "  patches_nsy = patches(img_nsy,256)\n",
    "  \n",
    "  path = y_test_dataframe.at[i, \"cleaned_image_path\"]\n",
    "  img_gt = cv2.imread(path)\n",
    "  img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
    "  img_gt = cv2.resize(img_gt,(1024,1024))  #resizing the y_test images\n",
    "  patches_gt = patches(img_gt,256)\n",
    "\n",
    "  rows = patches_nsy.shape[0]\n",
    "  cols = patches_nsy.shape[1]\n",
    "  for j in range(rows):\n",
    "    for k in range(cols):\n",
    "      X_test_patches.append(patches_nsy[j][k][0])\n",
    "      y_test_patches.append(patches_gt[j][k][0])\n",
    "  \n",
    "X_test_dataframe = np.array(X_test_patches)\n",
    "y_test_dataframe = np.array(y_test_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25385cb-2071-49dc-bac7-7ffab77ddbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls denoising-shabby-pages/train/cleaned/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9e3e1-c88a-4e21-b608-dbecd4302715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating patches for X_test and y_test\n",
    "X_train_patches = []\n",
    "y_train_patches = []\n",
    "for i in range(len(X_train_dataframe)):\n",
    "  path = X_train_dataframe.at[i, \"shabby_image_path\"]\n",
    "  img_gt = cv2.imread(path)\n",
    "  if img_gt is None:\n",
    "        print(f\"Error loading image: {path}\")\n",
    "  else:\n",
    "    # Proceed with further operations\n",
    "\n",
    "      img_nsy = cv2.imread(path)\n",
    "      img_nsy = cv2.cvtColor(img_nsy, cv2.COLOR_BGR2RGB)\n",
    "      img_nsy = cv2.resize(img_nsy,(1024,1024))  #resizing the X_train images\n",
    "      patches_nsy = patches(img_nsy,256)\n",
    "\n",
    "  path = y_train_dataframe.at[i, \"cleaned_image_path\"]\n",
    "  img_gt = cv2.imread(path)\n",
    "  if img_gt is None:\n",
    "        print(f\"Error loading image: {path}\")\n",
    "  else:\n",
    "      img_gt = cv2.imread(path)\n",
    "      img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
    "      img_gt = cv2.resize(img_gt,(1024,1024))  #resizing the y_train images\n",
    "      patches_gt = patches(img_gt,256)\n",
    "\n",
    "  rows = patches_nsy.shape[0]\n",
    "  cols = patches_nsy.shape[1]\n",
    "  for j in range(rows):\n",
    "     for k in range(cols):\n",
    "          X_train_patches.append(patches_nsy[j][k][0])\n",
    "          y_train_patches.append(patches_gt[j][k][0])\n",
    "\n",
    "X_train_dataframe = np.array(X_train_patches)\n",
    "y_train_dataframe = np.array(y_train_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfde078-2a20-4fd0-a362-ecdc0fb09498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating patches for X_validate and y_validate\n",
    "X_valid_patches = []\n",
    "y_valid_patches = []\n",
    "for i in range(len(X_validate_dataframe)):\n",
    "  path = X_validate_dataframe.at[i, \"shabby_image_path\"]\n",
    "  img_nsy = cv2.imread(path)\n",
    "  img_nsy = cv2.cvtColor(img_nsy, cv2.COLOR_BGR2RGB)\n",
    "  img_nsy = cv2.resize(img_nsy,(1024,1024))  #resizing the X_validate images\n",
    "  patches_nsy = patches(img_nsy,256)\n",
    "  \n",
    "  path = y_validate_dataframe.at[i, \"cleaned_image_path\"]\n",
    "  img_gt = cv2.imread(path)\n",
    "  img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
    "  img_gt = cv2.resize(img_gt,(1024,1024))  #resizing the y_validate images\n",
    "  patches_gt = patches(img_gt,256)\n",
    "\n",
    "  rows = patches_nsy.shape[0]\n",
    "  cols = patches_nsy.shape[1]\n",
    "  for j in range(rows):\n",
    "    for k in range(cols):\n",
    "      X_valid_patches.append(patches_nsy[j][k][0])\n",
    "      y_valid_patches.append(patches_gt[j][k][0])\n",
    "  \n",
    "X_validate_dataframe = np.array(X_valid_patches)\n",
    "y_validate_dataframe = np.array(y_valid_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f8073-cd53-4861-ac0d-3a641bbc20b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train_dataframe.shape)\n",
    "print(y_train_dataframe.shape)\n",
    "print(X_test_dataframe.shape)\n",
    "print(y_test_dataframe.shape)\n",
    "print(X_validate_dataframe.shape)\n",
    "print(y_validate_dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1545aac-4837-4ee6-8783-0a6dcfbb413e",
   "metadata": {},
   "source": [
    "plot patches for both clean and shabby images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e419c3-7d74-4461-ad4b-513947a571e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "fig, axs = plt.subplots(2,5,figsize=(20,10))\n",
    "r = random.sample(range(0, 6911), 5)\n",
    "\n",
    "fig.suptitle('Train Image Patches',fontweight =\"bold\")\n",
    "for i in range(5):\n",
    "  axs[0][i].imshow(y_train_dataframe[r[i]])\n",
    "  axs[0][i].set_title('Ground Truth Image Patches')\n",
    "  axs[1][i].imshow(X_train_dataframe[r[i]])\n",
    "  axs[1][i].set_title('Noisy Image Patches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d701f9c-7940-4c8a-b70f-aa005cee6baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total number of image patches on train data : \", len(X_train_dataframe))\n",
    "print(\"Total number of image patches on test data : \", len(X_test_dataframe))\n",
    "print(\"Total number of image patches on validate data : \", len(X_validate_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14be5fd-fcd2-46a4-a841-3742b73d110d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_red_gt = []\n",
    "mean_blue_gt = []\n",
    "mean_green_gt = []\n",
    "mean_red_nsy = []\n",
    "mean_blue_nsy = []\n",
    "mean_green_nsy = []\n",
    "for path in test_df['cleaned_image_path']:\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  mean_red_gt.append(np.mean(img[:,:,0]))\n",
    "  mean_green_gt.append(np.mean(img[:,:,1]))\n",
    "  mean_blue_gt.append(np.mean(img[:,:,2]))\n",
    "\n",
    "for path in test_df['shabby_image_path']:\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  mean_red_nsy.append(np.mean(img[:,:,0]))\n",
    "  mean_green_nsy.append(np.mean(img[:,:,1]))\n",
    "  mean_blue_nsy.append(np.mean(img[:,:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f13f1e-7b78-4aeb-95c3-4fbd2416bcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "red_gt = pd.DataFrame()\n",
    "green_gt = pd.DataFrame()\n",
    "blue_gt = pd.DataFrame()\n",
    "red_nsy = pd.DataFrame()\n",
    "green_nsy = pd.DataFrame()\n",
    "blue_nsy = pd.DataFrame()\n",
    "\n",
    "red_gt['Mean Pixel on Ground Truth Images'] = mean_red_gt\n",
    "red_gt['channel'] = 'red'\n",
    "red_nsy['Mean Pixel on  Noisy Images'] = mean_red_nsy\n",
    "red_nsy['channel'] = 'red'\n",
    "\n",
    "green_gt['Mean Pixel on Ground Truth Images'] = mean_green_gt\n",
    "green_gt['channel'] = 'green'\n",
    "green_nsy['Mean Pixel on  Noisy Images'] = mean_green_nsy\n",
    "green_nsy['channel'] = 'green'\n",
    "\n",
    "blue_gt['Mean Pixel on Ground Truth Images'] = mean_blue_gt\n",
    "blue_gt['channel'] = 'blue'\n",
    "blue_nsy['Mean Pixel on  Noisy Images'] = mean_blue_nsy\n",
    "blue_nsy['channel'] = 'blue'\n",
    "\n",
    "concat_gt = pd.concat([red_gt,green_gt,blue_gt],ignore_index=True)\n",
    "concat_nsy = pd.concat([red_nsy,green_nsy,blue_nsy],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048157c-7343-4ecb-912e-8742fa0eda7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of mean pixels of images\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(3,2,figsize=(16, 16))\n",
    "fig.suptitle(\"Ground Truth Images\", fontsize = 'x-large' , fontweight = 'bold' )\n",
    "sns.histplot(mean_red_gt,ax=axes[0][0],color='r')\n",
    "sns.distplot(mean_red_gt,ax=axes[0][1],hist=False,color='r')\n",
    "axes[0][0].set_xlabel('Mean Pixels')\n",
    "axes[0][1].set_xlabel('Mean Pixels')\n",
    "\n",
    "sns.histplot(mean_green_gt,ax=axes[1][0],color='g')\n",
    "sns.distplot(mean_green_gt,ax=axes[1][1],hist=False,color='g')\n",
    "axes[1][0].set_xlabel('Mean Pixels')\n",
    "axes[1][1].set_xlabel('Mean Pixels')\n",
    "\n",
    "sns.histplot(mean_blue_gt,ax=axes[2][0],color='b')\n",
    "sns.distplot(mean_blue_gt,ax=axes[2][1],hist=False,color='b')\n",
    "axes[2][0].set_xlabel('Mean Pixels')\n",
    "axes[2][1].set_xlabel('Mean Pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0e895-cc67-4fe3-ad83-9d8884e18884",
   "metadata": {},
   "source": [
    "Obsevations: for most of the clean images, the mean pixel values ranges between 210 to 250. This means, most of the images have dark to medium brightness. Only few images have high mean pixel values or high brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a413cd-0970-42ec-83b6-e8b252f115b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2,figsize=(16, 16))\n",
    "fig.suptitle(\"Noisy Images\", fontsize = 'x-large' , fontweight = 'bold' )\n",
    "sns.histplot(mean_red_nsy,ax=axes[0][0],color='r')\n",
    "sns.distplot(mean_red_nsy,ax=axes[0][1],hist=False,color='r')\n",
    "axes[0][0].set_xlabel('Mean Pixels')\n",
    "axes[0][1].set_xlabel('Mean Pixels')\n",
    "\n",
    "sns.histplot(mean_green_nsy,ax=axes[1][0],color='g')\n",
    "sns.distplot(mean_green_nsy,ax=axes[1][1],hist=False,color='g')\n",
    "axes[1][0].set_xlabel('Mean Pixels')\n",
    "axes[1][1].set_xlabel('Mean Pixels')\n",
    "\n",
    "sns.histplot(mean_blue_nsy,ax=axes[2][0],color='b')\n",
    "sns.distplot(mean_blue_nsy,ax=axes[2][1],hist=False,color='b')\n",
    "axes[2][0].set_xlabel('Mean Pixels')\n",
    "axes[2][1].set_xlabel('Mean Pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fe2fd-42cb-49ff-91aa-11a974aa1a98",
   "metadata": {},
   "source": [
    "Obsevations: for most of the clean images, the mean pixel values ranges between 210 to 250. This means, most of the images have dark to medium brightness. Only few images have high mean pixel values or high brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647f0ee-4104-47b1-b256-3db49f620280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2,figsize=(16, 16))\n",
    "fig.suptitle(\"Noisy Images\", fontsize = 'x-large' , fontweight = 'bold' )\n",
    "sns.histplot(mean_red_nsy,ax=axes[0][0],color='r')\n",
    "sns.distplot(mean_red_nsy,ax=axes[0][1],hist=False,color='r')\n",
    "axes[0][0].set_xlabel('Mean Pixels')\n",
    "axes[0][1].set_xlabel('Mean Pixels')\n",
    "\n",
    "sns.histplot(mean_green_nsy,ax=axes[1][0],color='g')\n",
    "sns.distplot(mean_green_nsy,ax=axes[1][1],hist=False,color='g')\n",
    "axes[1][0].set_xlabel('Mean Pixels')\n",
    "axes[1][1].set_xlabel('Mean Pixels')\n",
    "\n",
    "sns.histplot(mean_blue_nsy,ax=axes[2][0],color='b')\n",
    "sns.distplot(mean_blue_nsy,ax=axes[2][1],hist=False,color='b')\n",
    "axes[2][0].set_xlabel('Mean Pixels')\n",
    "axes[2][1].set_xlabel('Mean Pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5373ac4-63a8-4197-b5ce-58315adc3b31",
   "metadata": {},
   "source": [
    "Obsevations: for most of the clean images, the mean pixel values ranges between 150 to 250. This means, most of the images have dark to medium brightness. Only few images have high mean pixel values or high brightness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1822b1c-62fe-4b3b-8ad1-595834149762",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyzing the PSNR and SSIM values of the images\n",
    "The Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are both widely used metrics for evaluating the quality of images, particularly in the context of image compression and restoration.\n",
    "PSNR measures the ratio between the maximum possible power of a signal (in this case, an image) and the power of corrupting noise that affects the fidelity of its representation.\n",
    "Higher PSNR values indicate higher image quality\n",
    "SSIM is designed to measure the structural similarity between two images, considering luminance, contrast, and structure. Unlike PSNR, SSIM aims to reflect perceived image quality more accurately.\n",
    " It outputs a value between -1 and 1, where 1 indicates perfect similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6f629-0d4c-49c0-9413-5879a7f546c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "from tqdm import tqdm\n",
    "SSIM = [];PSNR = [];\n",
    "for i in tqdm(range(len(train_df))):\n",
    "  path = train_df['cleaned_image_path'].iloc[i]\n",
    "  img1 = cv2.imread(path)\n",
    "  img1 = img1.astype(\"float32\") / 255.0\n",
    "  path = train_df['shabby_image_path'].iloc[i]\n",
    "  img2 = cv2.imread(path)\n",
    "  img2 = img2.astype(\"float32\") / 255.0\n",
    "  window_size = 3\n",
    "  SSIM.append(ssim(img1,img2,multichannel=True,win_size=window_size, data_range=img2.max() - img2.min()))\n",
    "  PSNR.append(psnr(img1,img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4d307-84b6-4259-bfb4-05f4818572b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.displot(PSNR,kind='kde')\n",
    "ax.set(xlabel='PSNR', ylabel='Density')\n",
    "ax = sns.displot(PSNR)\n",
    "ax.set(xlabel='PSNR', ylabel='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9d762-4dc5-4b07-9de7-91f0a64e110a",
   "metadata": {},
   "source": [
    "Observations : Majority of the clean-noisy image pairs have PSNR value between 10-15. So, a good denoising model should give PSNR value greater than 15 (approx) for majority of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e8079-54dd-4c6a-92c7-6b6c931a9ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ax = sns.displot(SSIM,kind='kde')\n",
    "ax.set(xlabel='SSIM', ylabel='Density')\n",
    "ax = sns.displot(SSIM)\n",
    "ax.set(xlabel='SSIM', ylabel='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1164a-4170-4291-8d2b-a98c1b65d441",
   "metadata": {},
   "source": [
    "Observations : Majority of the clean-noisy image pairs have SSIM value between 0.5-0.8. So, a good denoising model should give SSIM value greater than 0.8 (approx) for majority of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8299166-e415-4de2-8ace-f2b714bcf28d",
   "metadata": {},
   "source": [
    "# Denoising few image patches using NLM filter\n",
    "\n",
    "The basic idea behind the non-local means algorithm is to exploit redundancy in natural images. Instead of averaging pixel values within a local neighborhood as traditional filtering methods do, NLM looks for similar patches throughout the entire image. It averages the pixel values of similar patches, giving more weight to patches that are more similar to the one being denoised.\n",
    "Overall, the NLM denoising technique works by comparing small patches in the image to find similar patches across the entire image and averaging their pixel values to reduce noise while preserving details. The specific parameter values you choose will depend on your preferences and the characteristics of your images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2443f2-c02e-4c98-a86a-7601ae237360",
   "metadata": {},
   "source": [
    "Here's a simplified explanation of how the algorithm works:\n",
    "\n",
    "Patch Search: For each pixel in the image, a search is conducted to find similar patches. Similarity is usually measured using a distance metric, often based on the Euclidean distance between pixel values.\n",
    "\n",
    "Patch Weighting: Once similar patches are found, their pixel values are weighted based on their similarity to the current patch. Patches that are more similar have higher weights.\n",
    "\n",
    "Weighted Averaging: The pixel values of the similar patches are averaged, with more weight given to patches that are more similar. This averaging process reduces noise while preserving the underlying structure of the image.\n",
    "\n",
    "Reconstruction: The weighted averages of the pixel values are used to replace the noisy pixel values in the image, resulting in a denoised version of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd53b43-eee4-470b-95ae-e84cc7f9a359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Assuming patches_nsy and patches_gt are your arrays of noisy and ground truth image patches\n",
    "# rows and cols are the dimensions of your patches arrays\n",
    "\n",
    "num_samples = 10  # Number of random image pairs to visualize and analyze\n",
    "\n",
    "fig, axs = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
    "\n",
    "PSNR_nsy=[]\n",
    "PSNR_de_nsy=[]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    r = random.randint(0, rows - 1)\n",
    "    c = random.randint(0, cols - 1)\n",
    "\n",
    "    img1 = patches_nsy[r][c][0]\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2 = patches_gt[r][c][0]\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    axs[i][0].imshow(img1)\n",
    "    axs[i][0].set_title('Noisy Image Patch')\n",
    "\n",
    "    axs[i][1].imshow(img2)\n",
    "    axs[i][1].set_title('Ground Truth Image Patch')\n",
    "\n",
    "    # NLM Denoising\n",
    "    \"\"\" img1: The input noisy image that you want to denoise.\n",
    "        None: The optional output destination. In this case, None indicates that the output will be returned by the function.\n",
    "        50: The parameter h (h for \"hazardousness\") determines the filter strength. Higher values will remove noise effectively but might also remove image details.\n",
    "        Lower values will preserve details but might not remove noise as well.\n",
    "        10: The parameter hForColorComponents is similar to h but for color images. It specifies the strength of the filter for color channels.\n",
    "        A lower value will preserve more color details.\n",
    "        7: The size of the window used for searching similar patches. Larger values can remove noise more effectively but might remove finer details.\n",
    "        21: The size of the window used for averaging similar patches. Larger values will yield smoother results but might blur important details. \"\"\"\n",
    "    dst = cv2.fastNlMeansDenoisingColored(img1, None, 50, 10, 7, 21)\n",
    "    axs[i][2].imshow(dst)\n",
    "    axs[i][2].set_title('Denoised Image Patch')\n",
    "\n",
    "    # Calculate and print PSNR values\n",
    "    PSNR_nsy.append(psnr(img1,img2))\n",
    "    PSNR_de_nsy.append(psnr(img1,dst))\n",
    "    print(f\"Pair {i+1} - PSNR value between Noisy and Ground Truth patches:\", PSNR_nsy)\n",
    "    print(f\"Pair {i+1} - PSNR value between Noisy and Denoised patches:\", PSNR_de_nsy)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3238bf7-0ef8-46a6-ae67-df293a1a30a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "improvement = [x1 - x2 for (x1, x2) in zip(PSNR_de_nsy, PSNR_nsy)]\n",
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.add_column(\"PSNR before denoising\",PSNR_nsy)\n",
    "x.add_column(\"PSNR after denoising\",PSNR_de_nsy)\n",
    "x.add_column(\"PSNR Improvement\",improvement)\n",
    "print(x)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08241e-d023-4cdb-95b6-9b442ce7b112",
   "metadata": {},
   "source": [
    "Observations:\n",
    "As you can see, the NLM filter is able to denoise the images to some extent. But it smoothens many details that are present in the ground truth images leading to loss of important informations that should have been retained. Also, when noise is too high NLM fails to provide good results.\n",
    "\n",
    "Thus we can conclude, there is a need of using more advanced deep learning techniques for image denoising tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be463c5f-d7f1-4b36-8f9b-4d9b635f9158",
   "metadata": {},
   "source": [
    "Creating Dataset for modeling using custom data generators in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522d0de-5ae1-4321-a828-25f7c8776589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train_dataframe.shape)\n",
    "print(y_train_dataframe.shape)\n",
    "print(X_test_dataframe.shape)\n",
    "print(y_test_dataframe.shape)\n",
    "print(X_validate_dataframe.shape)\n",
    "print(y_validate_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf4732-21e5-451a-a8c2-cd7c6f83d3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalizing the image pixels\n",
    "X_train_dataframe = X_train_dataframe.astype(\"float32\") / 255.0\n",
    "y_train_dataframe = y_train_dataframe.astype(\"float32\") / 255.0\n",
    "X_test_dataframe = X_test_dataframe.astype(\"float32\") / 255.0\n",
    "y_test_dataframe = y_test_dataframe.astype(\"float32\") / 255.0\n",
    "X_validate_dataframe = X_validate_dataframe.astype(\"float32\") / 255.0\n",
    "y_validate_dataframe = y_validate_dataframe.astype(\"float32\") / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d234afe-d933-4159-afc0-f1d3a85bcdec",
   "metadata": {},
   "source": [
    "This code snippet defines a custom data loader class that efficiently loads batches of data and labels for training, validation, and testing purposes. This class is useful when working with large datasets that cannot be loaded into memory all at once. It ensures that neural network training processes can iterate over the data efficiently in a batch-wise manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ebb9a-2ee6-463e-b976-5513ab4469de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# This class inherits from the Keras Sequence class, which is used to work with datasets that are too large to fit in memory at once.\n",
    "class Dataloder(tf.keras.utils.Sequence): \n",
    "    # The constructor initializes the data loader object\n",
    "    def __init__(self, X,y,batch_size=1, shuffle=False):\n",
    "        # The input data (features)\n",
    "        self.X = X\n",
    "        # The corresponding labels\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # An array containing the indices of the data samples\n",
    "        self.indexes = np.arange(len(X))\n",
    "    # This method is used to retrieve batches of data and labels given an index i.\n",
    "    # It slices the input data and labels arrays to create a batch of size batch_size for both data and labels.\n",
    "    # It then returns a tuple containing the batch of input data and the corresponding batch of labels.\n",
    "    #. the output: (Data: [1 2 3 4 5], Labels: [1 4 9 16 25])\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch data\n",
    "        batch_x = self.X[i * self.batch_size : (i+1) * self.batch_size]\n",
    "        batch_y = self.y[i * self.batch_size : (i+1) * self.batch_size]\n",
    "        \n",
    "        return tuple((batch_x,batch_y))\n",
    "    # This method returns the number of batches in the data loader\n",
    "    def __len__(self):\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b412ef5-6892-436a-bebc-98496ee0ea3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_dataloader = Dataloder(X_train_dataframe,y_train_dataframe, batch_size, shuffle=True)\n",
    "validate_dataloader = Dataloder(X_validate_dataframe,y_validate_dataframe,batch_size, shuffle=True)\n",
    "test_dataloader = Dataloder(X_test_dataframe,y_test_dataframe,batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34eb03-17d2-42f1-af81-ebf50b76fa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d142128-472f-487e-8e11-f0768d52292d",
   "metadata": {},
   "source": [
    "Baseline Model : Autoencoder\n",
    "This is a simple encoder decoder network with 3 convolutional layers followed by max pooling for encoders and 3 deconvolutional layers for decoders. The output from decoder is then given to a convolutional layer with 3 filters to maintain the similar input and output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e61147-2594-4d2b-a14a-62336ae52a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation, Flatten, Dense, Input, MaxPooling2D, Add, Reshape, concatenate, AveragePooling2D, Multiply, GlobalAveragePooling2D, UpSampling2D, MaxPool2D,Softmax\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648db93-06dc-4d8a-9d7a-6646fcdc4602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://keras.io/examples/vision/autoencoder/\n",
    "tf.keras.backend.clear_session()\n",
    "input = Input(shape=(256, 256, 3))\n",
    "\n",
    "# Encoder\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(input)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\",kernel_initializer='he_normal', padding=\"same\")(x)\n",
    "x = Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = Conv2D(3, (3, 3), activation=\"sigmoid\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, x)\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b85e0-ed08-4448-9a5e-39cc870a6b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Specifies the directory where TensorBoard logs will be stored. TensorBoard is a tool that helps visualize training metrics, model architectures, and more.\n",
    "# log_dir = \"logs/model_1\"\n",
    "# tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "# # The ReduceLROnPlateau technique, which adjusts the learning rate (LR) when the model's validation loss plateaus\n",
    "# reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, patience=2)\n",
    "# callback = [tensorboard, reducelr]\n",
    "# autoencoder.fit(train_dataloader, shuffle=True, epochs=15, validation_data=test_dataloader, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfc72d-11e0-40c2-9294-27a48b01a336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autoencoder.save('autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8d3b7-e4a1-4267-bc85-41bdd81bc07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "autoencoder =  tf.keras.models.load_model('autoencoder.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b95950-c4ba-4c25-a053-dfa314f383a9",
   "metadata": {},
   "source": [
    "prediction_tflite is tailored to work with quantized TFLite models, while prediction is designed for general Keras models. The choice between the two functions depends on the type of model you have trained and intend to use for denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8cd60-98ba-48b3-b968-468edd8dc688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Custom function to get denoised image prediction for noisy images\n",
    "def prediction(img,model):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = cv2.resize(img,(1024,1024))\n",
    "  img = img.astype(\"float32\") / 255.0\n",
    "\n",
    "  img_patches = patches(img,256)\n",
    "\n",
    "  nsy=[]\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      nsy.append(img_patches[i][j][0])\n",
    "  nsy = np.array(nsy)\n",
    "\n",
    "  pred_img = model.predict(nsy)\n",
    "  pred_img = np.reshape(pred_img,(4,4,1,256,256,3))\n",
    "  pred_img = unpatchify(pred_img, img.shape)\n",
    "  return pred_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2362e44-e117-43fd-95a6-fbb60df28de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Custom function to get denoised image prediction for noisy images on quantized models using tflite\n",
    "def prediction_tflite(img,model):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = cv2.resize(img,(1024,1024))\n",
    "  img = img.astype(\"float32\") / 255.0\n",
    "\n",
    "  img_patches = patches(img,256)\n",
    "\n",
    "  nsy=[]\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      nsy.append(img_patches[i][j][0])\n",
    "  nsy = np.array(nsy)\n",
    "  pred=[]\n",
    "  for patch in nsy:\n",
    "    model.set_tensor(input_details[0]['index'], tf.expand_dims(patch,axis=0))\n",
    "    model.invoke()\n",
    "    tflite_model_predictions = model.get_tensor(output_details[0]['index'])\n",
    "    pred.append(tflite_model_predictions)\n",
    "\n",
    "  pred_img = np.reshape(pred,(4,4,1,256,256,3))\n",
    "  pred_img = unpatchify(pred_img, img.shape)\n",
    "  return pred_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d5f51-2dce-4921-b75b-5a9fa9b85f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee5ace-610b-446d-b90a-f0869c512baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Custom function to plot/visualize noisy, ground truth and predicted images\n",
    "def visualize(sample,model):\n",
    "  fig,ax = plt.subplots(len(sample),3,figsize=(30,30))\n",
    "  for i in range(len(sample)):\n",
    "    path = sample['cleaned_image_path'].iloc[i]\n",
    "    test_img_gt = cv2.imread(path)\n",
    "    test_img_gt = cv2.cvtColor(test_img_gt, cv2.COLOR_BGR2RGB)\n",
    "    test_img_gt = cv2.resize(test_img_gt,(512,512))\n",
    "    test_img_gt = test_img_gt.astype(\"float32\") / 255.0\n",
    "  \n",
    "    path = sample['shabby_image_path'].iloc[i]\n",
    "    test_img_nsy = cv2.imread(path)\n",
    "    pred_img = prediction(test_img_nsy,model)\n",
    "    pred_img = cv2.resize(pred_img,(512,512))\n",
    "\n",
    "    test_img_nsy = cv2.cvtColor(test_img_nsy, cv2.COLOR_BGR2RGB)\n",
    "    test_img_nsy = cv2.resize(test_img_nsy,(512,512))\n",
    "    test_img_nsy = test_img_nsy.astype(\"float32\") / 255.0\n",
    "    \n",
    "    ax[i][0].imshow(test_img_nsy)\n",
    "    ax[i][0].get_xaxis().set_visible(False)\n",
    "    ax[i][0].get_yaxis().set_visible(False)\n",
    "    ax[i][0].title.set_text(\"Noisy Image\")\n",
    "\n",
    "    ax[i][1].imshow(test_img_gt)\n",
    "    ax[i][1].get_xaxis().set_visible(False)\n",
    "    ax[i][1].get_yaxis().set_visible(False)\n",
    "    ax[i][1].title.set_text(\"Ground Truth Image\")\n",
    "\n",
    "    ax[i][2].imshow(pred_img)\n",
    "    ax[i][2].get_xaxis().set_visible(False)\n",
    "    ax[i][2].get_yaxis().set_visible(False)\n",
    "    ax[i][2].title.set_text(\"Predicted Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651b565-a04b-41f1-bfdb-2d3b20ff9ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = pd.DataFrame({'cleaned_image_path': ['denoising-shabby-pages/train/cleaned/0001-USPS-dmm300_608.pdf-15.png',  'denoising-shabby-pages/train/cleaned/0001-USPS-dmm300_608.pdf-18.png', 'denoising-shabby-pages/train/cleaned/0002-HHS-ocse_eiwo_paperless_solution_presentation.pdf-01.png'], 'shabby_image_path': ['denoising-shabby-pages/train/shabby/0001-USPS-dmm300_608.pdf-15.png', 'denoising-shabby-pages/train/shabby/0001-USPS-dmm300_608.pdf-18.png', 'denoising-shabby-pages/train/shabby/0002-HHS-ocse_eiwo_paperless_solution_presentation.pdf-01.png']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd7e89-68f7-4ac2-9050-03173cfdbea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize(sample,autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba82244-dafb-4331-911a-0d036ae6290a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25faebe4-0a30-41bb-9b0a-a0ee1a40d4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def psnr_and_ssim(test_df, model, model_type='Normal'):\n",
    "    psnr_nsy = 0.0\n",
    "    psnr_de_nsy = 0.0\n",
    "    ssim_nsy = 0.0\n",
    "    ssim_de_nsy = 0.0\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "        # Getting the noisy image path\n",
    "        nsy_path = test_df['shabby_image_path'].iloc[i]\n",
    "\n",
    "        # Load the noisy image\n",
    "        nsy = cv2.imread(nsy_path)\n",
    "        if nsy is None:\n",
    "            print(f\"Error loading noisy image: {nsy_path}\")\n",
    "            continue  # Skip this iteration and move to the next\n",
    "        \n",
    "        # Getting the predicted images\n",
    "        if model_type == 'Quantized': \n",
    "            pred = prediction_tflite(nsy, model)\n",
    "        else:\n",
    "            pred = prediction(nsy, model)\n",
    "\n",
    "        # Getting the ground truth image data\n",
    "        gt_path = test_df['cleaned_image_path'].iloc[i]\n",
    "        gt = cv2.imread(gt_path)\n",
    "        if gt is None:\n",
    "            print(f\"Error loading ground truth image: {gt_path}\")\n",
    "            continue  # Skip this iteration and move to the next\n",
    "        gt = cv2.cvtColor(gt, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resizing the images\n",
    "        gt = cv2.resize(gt, (1024, 1024))\n",
    "        nsy = cv2.resize(nsy, (1024, 1024))\n",
    "\n",
    "        # Normalizing the images\n",
    "        gt = gt.astype(\"float32\") / 255.0\n",
    "        nsy = nsy.astype(\"float32\") / 255.0\n",
    "\n",
    "        # Computing PSNR and SSIM for test images\n",
    "        psnr_nsy += psnr(gt, nsy)\n",
    "        psnr_de_nsy += psnr(gt, pred)\n",
    "        ssim_nsy += ssim(gt, nsy, multichannel=True, data_range=nsy.max() - nsy.min())\n",
    "        ssim_de_nsy += ssim(gt, pred, multichannel=True, data_range=pred.max() - pred.min())\n",
    "\n",
    "    psnr_nsy = psnr_nsy / len(test_df)\n",
    "    psnr_de_nsy = psnr_de_nsy / len(test_df)\n",
    "    ssim_nsy = ssim_nsy / len(test_df)\n",
    "    ssim_de_nsy = ssim_de_nsy / len(test_df)\n",
    "    return psnr_nsy, psnr_de_nsy, ssim_nsy, ssim_de_nsy\n",
    "\n",
    "# Calculate and print PSNR and SSIM\n",
    "psnr_nsy, psnr_de_nsy, ssim_nsy, ssim_de_nsy = psnr_and_ssim(test_df, autoencoder)\n",
    "print('PSNR before denoising:', psnr_nsy)\n",
    "print('PSNR after denoising:', psnr_de_nsy)\n",
    "print('SSIM before denoising:', ssim_nsy)\n",
    "print('SSIM after denoising:', ssim_de_nsy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99812775-8a83-4037-8d5b-2c400bb6b25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_size = round((os.stat('autoencoder.h5').st_size)/(1024**2),3)\n",
    "PSNR = [];SSIM = [];PSNR_imp = [];SSIM_imp = [];size=[]\n",
    "PSNR.append(round(psnr_nsy,3))\n",
    "PSNR.append(round(psnr_de_nsy,3))\n",
    "PSNR_imp.append('-')\n",
    "PSNR_imp.append(round(psnr_de_nsy-psnr_nsy,3))\n",
    "\n",
    "SSIM.append(round(ssim_nsy,3))\n",
    "SSIM.append(round(ssim_de_nsy,3))\n",
    "SSIM_imp.append('-')\n",
    "SSIM_imp.append(round(ssim_de_nsy-ssim_nsy,3))\n",
    "\n",
    "size.append('-')\n",
    "size.append(model_size)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6736f90-9a9e-47f2-8e87-83f4815d70fe",
   "metadata": {},
   "source": [
    "# CBDNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd1fbd-096f-432a-bb2f-eaaacad0fd93",
   "metadata": {
    "tags": []
   },
   "source": [
    "CDBNet, or Convolutional Denoising Bilinear Network, has a specific architecture designed to effectively denoise images. Here's an overview of how it is typically built:\n",
    "\n",
    "1.Input Layer: The network takes a noisy image as input. This image contains the noise that needs to be removed.\n",
    "\n",
    "2. Convolutional Layers: CDBNet starts with several convolutional layers. These layers apply filters to the input image, extracting features and patterns. These filters identify both the noise and the underlying content of the image.\n",
    "\n",
    "3. Bilinear-Interpolated Image: Before reaching the Bilinear Fusion Layer, the input image goes through a process called bilinear interpolation. In this process, the noisy input image is upscaled to a higher resolution using interpolation techniques. This means that new pixel values are estimated based on the existing pixel values in the image. This upscaled image captures the high-frequency details present in the original image, even though it still contains noise.\n",
    "\n",
    "4. Residual Blocks: Residual blocks are often included to capture complex relationships between image features. These blocks allow the network to learn residual information, which helps in the denoising process.\n",
    "\n",
    "5. Skip Connections: Skip connections connect layers at different depths in the network. They allow information from earlier layers to bypass some layers and directly contribute to the final output. This helps prevent the vanishing gradient problem and aids in information flow.\n",
    "\n",
    "6. Bilinear Fusion Layer: This layer combines the bilinear-interpolated image with the output of the residual blocks. It helps in integrating the high-frequency details from the interpolated image with the denoised features.\n",
    "\n",
    "7. Convolutional Layers (again): After the fusion, additional convolutional layers are applied. These layers further refine the denoised image by processing the fused features.\n",
    "\n",
    "8. Output Layer: The final output is the denoised image. It should ideally resemble the clean version of the image without the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70816e1d-58d5-48be-9cc1-c3347701db56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/IDKiro/CBDNet-tensorflow/blob/dev/model.py\n",
    "tf.keras.backend.clear_session()\n",
    "input = Input(shape=(256, 256, 3))\n",
    "\n",
    "#Noise estimation subnetwork\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(input)\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "x = Conv2D(3, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "\n",
    "#Non Blind denoising subnetwork\n",
    "x = concatenate([x,input])\n",
    "conv1 = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(x)\n",
    "conv2 = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv1)\n",
    "\n",
    "pool1 = AveragePooling2D(pool_size=(2,2),padding='same')(conv2)\n",
    "conv3 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(pool1)\n",
    "conv4 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv3)\n",
    "conv5 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv4)\n",
    "\n",
    "pool2 = AveragePooling2D(pool_size=(2,2),padding='same')(conv5)\n",
    "conv6 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(pool2)\n",
    "conv7 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv6)\n",
    "conv8 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv7)\n",
    "conv9 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv8)\n",
    "conv10 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv9)\n",
    "conv11 = Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv10)\n",
    "\n",
    "upsample1 = Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv11)\n",
    "add1 = Add()([upsample1,conv5])\n",
    "conv12 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(add1)\n",
    "conv13 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv12)\n",
    "conv14 = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv13)\n",
    "\n",
    "upsample2 = Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv14)\n",
    "add1 = Add()([upsample2,conv2])\n",
    "conv15 = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(add1)\n",
    "conv16 = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv15)\n",
    "\n",
    "out = Conv2D(3, (1,1), kernel_initializer='he_normal',padding=\"same\")(conv16)\n",
    "out = Add()([out,input])\n",
    "\n",
    "CBDNet = Model(input,out)\n",
    "CBDNet.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "CBDNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78568e01-c343-4b2b-8fdf-8ac0267249fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# log_dir=\"logs/model_2\"\n",
    "# tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "# reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,verbose=1,patience=4,min_delta=0.00001)\n",
    "# callback = [tensorboard,reducelr]\n",
    "# CBDNet.fit(train_dataloader,shuffle=True,epochs=30,validation_data= test_dataloader,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219817e6-ce4d-4154-ad9c-fa1a3bd12a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CBDNet.save('CBDNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7ae3d-a2ec-4b46-9d4d-e7a9857009c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CBDNet = tf.keras.models.load_model('CBDNet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46371af-59fa-4858-a32f-55d4843a7d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize(sample,CBDNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1ec7c-5894-4484-89ff-9bfea70c6a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "psnr_nsy, psnr_de_nsy, ssim_nsy, ssim_de_nsy = psnr_and_ssim(test_df,CBDNet)\n",
    "print('PSNR before denoising :', psnr_nsy)\n",
    "print('PSNR after denoising :', psnr_de_nsy)\n",
    "print('SSIM before denoising :', ssim_nsy)\n",
    "print('SSIM after denoising :', ssim_de_nsy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc6e60-0788-487f-946e-fd899daa5f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_size = round((os.stat('CBDNet.h5').st_size)/(1024**2),3)\n",
    "PSNR.append(round(psnr_de_nsy,3))\n",
    "SSIM.append(round(ssim_de_nsy,3))\n",
    "PSNR_imp.append(round(psnr_de_nsy-psnr_nsy,3))\n",
    "SSIM_imp.append(round(ssim_de_nsy-ssim_nsy,3))\n",
    "size.append(model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5b623-252a-44ed-9ba7-b42a7cacbabc",
   "metadata": {},
   "source": [
    "#  PRIDNET\n",
    "(Progressive Residual Illumination Denoising Network) is an advanced neural network architecture designed for image denoising. It incorporates multiple stages to effectively reduce noise while preserving image details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fdf7f-0079-4df3-83c6-c459b026e609",
   "metadata": {
    "tags": []
   },
   "source": [
    "Noise Estimation Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9f3ad-7c2d-4850-8333-0ef32708d3d4",
   "metadata": {},
   "source": [
    "In this initial stage, PRIDNet estimates the noise present in the input noisy image. Accurate noise estimation is crucial for effective denoising, as it helps the network understand the characteristics of the noise that needs to be removed. PRIDNet uses a specific module to estimate the noise level, which is then used to guide subsequent denoising processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5da3e-a0b2-405c-9bd5-a7cbbb7f7554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/491506870/PRIDNet/blob/master/network.py\n",
    "class convolutional_block1(tf.keras.layers.Layer):\n",
    "    def __init__(self,filters,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters \n",
    "        self.conv1 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "        self.conv2 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "        self.conv3 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "        self.conv4 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "\n",
    "    def get_config(self):\n",
    "      config = super().get_config().copy()\n",
    "      config.update({'filters': self.filters})\n",
    "      return config\n",
    "\n",
    "    def call(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.conv3(X)\n",
    "        X = self.conv4(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37018b-21dc-486a-8e06-f3f1991a28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CAM(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.gap = GlobalAveragePooling2D()\n",
    "    self.dense1 = Dense(units=2,activation='relu')\n",
    "    self.dense2 = Dense(units=32,activation='sigmoid')\n",
    "\n",
    "  def call(self, X):\n",
    "    Y = self.gap(X)\n",
    "    Y = self.dense1(Y)\n",
    "    Y = self.dense2(Y)\n",
    "    X = Multiply()([X,Y])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381d4c7-1f0e-4d06-91f6-b08897ef3549",
   "metadata": {},
   "source": [
    "Multi Stage Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583fae3-8976-4a16-a1a9-2d971a0fc899",
   "metadata": {},
   "source": [
    "PRIDNet employs a multi-stage approach to progressively refine the denoising process. Each denoising stage focuses on different aspects of noise reduction. The model uses multiple convolutional blocks, residual networks, or other architectural components to iteratively enhance the denoising quality. The intermediate outputs from these stages serve as inputs for subsequent stages, allowing the model to build upon its denoising performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa362a-7105-4a0d-9bb8-24979830ac29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class convolutional_block2(tf.keras.layers.Layer):\n",
    "    def __init__(self,filters,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters \n",
    "        self.conv1 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "        self.conv2 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "        self.conv3 = Conv2D(filters = self.filters, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'filters': self.filters})\n",
    "        return config\n",
    "\n",
    "    def call(self, X):\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.conv3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23940db-ef30-4b38-aca1-96e2d31d8e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "class pyramid(tf.keras.layers.Layer):\n",
    "  def __init__(self,pool_size, upsample_size, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.pool_size = pool_size\n",
    "    self.upsample_size = upsample_size\n",
    "    self.upsample = UpSampling2D(self.upsample_size, interpolation='bilinear')\n",
    "    self.pool = AveragePooling2D(pool_size=(self.pool_size,self.pool_size))\n",
    "    \n",
    "    self.conv1 = convolutional_block1(filters=32)\n",
    "    self.maxpool1 = MaxPool2D(pool_size=[2, 2], padding='same')\n",
    "\n",
    "    self.conv2 = convolutional_block1(filters=64)\n",
    "    self.maxpool2 = MaxPool2D(pool_size=[2, 2], padding='same')\n",
    "\n",
    "    self.conv3 = convolutional_block1(filters=128)\n",
    "    self.maxpool3 = MaxPool2D(pool_size=[2, 2], padding='same')\n",
    "\n",
    "    self.conv4 = convolutional_block1(filters=256)\n",
    "    self.maxpool4 = MaxPool2D(pool_size=[2, 2], padding='same')\n",
    "\n",
    "    self.conv5 = convolutional_block1(filters=512)\n",
    "    \n",
    "    self.upsample1 = Conv2DTranspose(256, (3, 3), strides=2,kernel_initializer='he_normal',padding=\"same\")\n",
    "    self.conv6 = convolutional_block2(filters=256)\n",
    "\n",
    "    self.upsample2 = Conv2DTranspose(128, (3, 3), strides=2,kernel_initializer='he_normal',padding=\"same\")\n",
    "    self.conv7 = convolutional_block2(filters=128)\n",
    "    \n",
    "    self.upsample3 = Conv2DTranspose(64, (3, 3), strides=2,kernel_initializer='he_normal',padding=\"same\")\n",
    "    self.conv8 = convolutional_block2(filters=64)\n",
    "\n",
    "    self.upsample4 = Conv2DTranspose(32, (3, 3), strides=2,kernel_initializer='he_normal',padding=\"same\")\n",
    "    self.conv9 = convolutional_block2(filters=32)\n",
    "\n",
    "    self.conv10 = Conv2D(filters = 3, kernel_size=1,padding='same',kernel_initializer='he_normal')\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({'pool_size': self.pool_size,'upsample_size':self.upsample_size})\n",
    "    return config\n",
    "\n",
    "  def call(self, input):\n",
    "    conv1 = self.pool(input)\n",
    "    \n",
    "    conv1 = self.conv1(conv1)\n",
    "    pool1 = self.maxpool1(conv1)\n",
    "    \n",
    "    conv2 = self.conv2(pool1)\n",
    "    pool2 = self.maxpool2(conv2)\n",
    "\n",
    "    conv3 = self.conv3(pool2)\n",
    "    pool3 = self.maxpool3(conv3)\n",
    "\n",
    "    conv4 = self.conv4(pool3)\n",
    "    pool4 = self.maxpool4(conv4)\n",
    "\n",
    "    conv5 = self.conv5(pool4)\n",
    "\n",
    "    up1 = self.upsample1(conv5)\n",
    "    concat1 = concatenate([up1,conv4])\n",
    "    conv6 = self.conv6(concat1)\n",
    "\n",
    "    up2 = self.upsample2(conv6)    \n",
    "    concat2 = concatenate([up2,conv3])\n",
    "    conv7 = self.conv7(concat2)\n",
    "\n",
    "\n",
    "    up3 = self.upsample3(conv7)\n",
    "    concat3 = concatenate([up3,conv2])\n",
    "    conv8 = self.conv8(concat3)\n",
    "\n",
    "    up4 = self.upsample4(conv8)\n",
    "    concat4 = concatenate([up4,conv1])\n",
    "    conv9 = self.conv9(concat4)\n",
    " \n",
    "    conv10 = self.conv10(conv9)\n",
    "    out = self.upsample(conv10)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338e771-9fc2-403c-80df-117bb9f590e2",
   "metadata": {},
   "source": [
    "Feature Fusion Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d065e6-29e5-4cd2-a028-0feb0de21d41",
   "metadata": {
    "tags": []
   },
   "source": [
    "After the multiple denoising stages, PRIDNet incorporates a feature fusion mechanism. This stage combines the denoised features extracted from different stages, often using skip connections or concatenation. The feature fusion step aims to gather the benefits of denoising at various stages, effectively capturing diverse aspects of noise and image content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aad19e-89c3-4f55-9bf9-09337cfc798c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class KSM(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.conv1 = Conv2D(filters = 21, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "    self.conv2 = Conv2D(filters = 21, kernel_size=5,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "    self.conv3 = Conv2D(filters = 21, kernel_size=7,activation='relu',padding='same',kernel_initializer='he_normal')\n",
    "    self.gap = GlobalAveragePooling2D()\n",
    "    self.fc1 = Dense(units=2,activation='relu')\n",
    "    self.fc2_1 = Dense(units=21)\n",
    "    self.fc2_2 = Dense(units=21)\n",
    "    self.fc2_3 = Dense(units=21)\n",
    "\n",
    "  def call(self, input):\n",
    "    conv1 = self.conv1(input)\n",
    "    conv2 = self.conv2(input)\n",
    "    conv3 = self.conv3(input)\n",
    "    sum = Add()([conv1,conv2,conv3])\n",
    "    gap =  self.gap(sum)\n",
    "    gap = tf.reshape(gap, [-1, 1, 1, 21])\n",
    "    fc1 = self.fc1(gap)\n",
    "    a1 = self.fc2_1(fc1)\n",
    "    a2 = self.fc2_2(fc1)\n",
    "    a3 = self.fc2_3(fc1)\n",
    "\n",
    "    before_softmax = concatenate([a1, a2, a3],1)\n",
    "    after_softmax = softmax(before_softmax,axis=1)\n",
    "\n",
    "    a1 = after_softmax[:, 0, :, :]\n",
    "    a1 = tf.reshape(a1, [-1, 1, 1, 21])\n",
    "\n",
    "    a2 = after_softmax[:, 1, :, :]\n",
    "    a2 = tf.reshape(a2, [-1, 1, 1, 21])\n",
    "\n",
    "    a3 = after_softmax[:, 2, :, :]\n",
    "    a3 = tf.reshape(a3, [-1, 1, 1, 21])\n",
    "\n",
    "    out1 = Multiply()([a1,conv1])\n",
    "    out2 = Multiply()([a2,conv2])\n",
    "    out3 = Multiply()([a3,conv3])\n",
    "    out = Add()([out1,out2,out3])\n",
    "    return out      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68521c-f741-4b33-b9ba-f05c7567a608",
   "metadata": {
    "tags": []
   },
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74beb72b-32a3-4009-ab1f-9fbdc6066007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input = Input(shape=(256, 256, 3))\n",
    "\n",
    "C1 = convolutional_block1(filters=32)(input)\n",
    "cam = CAM()(C1)\n",
    "C2 = Conv2D(filters = 3, kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(cam)\n",
    "concat1 = concatenate([C2,input])\n",
    "\n",
    "p1 = pyramid(pool_size=1,upsample_size=1)(concat1)\n",
    "p2 = pyramid(pool_size=2,upsample_size=2)(concat1)\n",
    "p3 = pyramid(pool_size=4,upsample_size=4)(concat1)\n",
    "p4 = pyramid(pool_size=8,upsample_size=8)(concat1)\n",
    "p5 = pyramid(pool_size=16,upsample_size=16)(concat1)\n",
    "\n",
    "concat2 = concatenate([p1,p2,p3,p4,p5,concat1])\n",
    "ksm = KSM()(concat2)\n",
    "out = Conv2D(filters = 3, kernel_size=1,padding='same',kernel_initializer='he_normal')(ksm)\n",
    "\n",
    "PRIDNet = Model(input,out)\n",
    "PRIDNet.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "PRIDNet.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f815b-b5b3-4852-8219-e434737302ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# log_dir=\"logs/model_3\"\n",
    "# tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "# reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,verbose=1,patience=3,min_delta=0.00001)\n",
    "# callback = [tensorboard,reducelr]\n",
    "# PRIDNet.fit(train_dataloader,shuffle=True,epochs=30,validation_data= test_dataloader,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567aae3-861d-4f58-9264-cae2a947dc4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PRIDNet.save('PRIDNet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60852bdd-6e6f-4ada-af0b-1a43db9a9657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRIDNet = tf.keras.models.load_model('PRIDNet.h5',custom_objects={'convolutional_block1':convolutional_block1, 'CAM':CAM,'convolutional_block2':convolutional_block2,'pyramid':pyramid,'KSM':KSM})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64df60-26a9-4e2a-b992-7e416c839a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize(sample,PRIDNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9308e7e-5dd0-4057-9ac8-f2f59c6554ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "psnr_nsy, psnr_de_nsy, ssim_nsy, ssim_de_nsy = psnr_and_ssim(test_df,PRIDNet)\n",
    "print('PSNR before denoising :', psnr_nsy)\n",
    "print('PSNR after denoising :', psnr_de_nsy)\n",
    "print('SSIM before denoising :', ssim_nsy)\n",
    "print('SSIM after denoising :', ssim_de_nsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626cac3-d338-40ee-825c-a07d6656cfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_size = round((os.stat('PRIDNet.h5').st_size)/(1024**2),3)\n",
    "PSNR.append(round(psnr_de_nsy,3))\n",
    "SSIM.append(round(ssim_de_nsy,3))\n",
    "PSNR_imp.append(round(psnr_de_nsy-psnr_nsy,3))\n",
    "SSIM_imp.append(round(ssim_de_nsy-ssim_nsy,3))\n",
    "size.append(model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6c2aa-65c2-4d83-bcb2-cb2c34325486",
   "metadata": {},
   "source": [
    "RIDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e100060-965f-4a9e-a8d0-7725a7017271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/saeed-anwar/RIDNet\n",
    "#In the above reference code, short skip connection in EAM network and skip conncetions in the overall network was not included. I will be adding those as well.   \n",
    "class EAM(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    \n",
    "    self.conv1 = Conv2D(64, (3,3), dilation_rate=1,padding='same',activation='relu')\n",
    "    self.conv2 = Conv2D(64, (3,3), dilation_rate=2,padding='same',activation='relu') \n",
    "\n",
    "    self.conv3 = Conv2D(64, (3,3), dilation_rate=3,padding='same',activation='relu')\n",
    "    self.conv4 = Conv2D(64, (3,3), dilation_rate=4,padding='same',activation='relu')\n",
    "\n",
    "    self.conv5 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
    "\n",
    "    self.conv6 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
    "    self.conv7 = Conv2D(64, (3,3),padding='same')\n",
    "\n",
    "    self.conv8 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
    "    self.conv9 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
    "    self.conv10 = Conv2D(64, (1,1),padding='same')\n",
    "\n",
    "    self.gap = GlobalAveragePooling2D()\n",
    "\n",
    "    self.conv11 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
    "    self.conv12 = Conv2D(64, (3,3),padding='same',activation='sigmoid')\n",
    "\n",
    "  def call(self,input):\n",
    "    conv1 = self.conv1(input)\n",
    "    conv1 = self.conv2(conv1)\n",
    "\n",
    "    conv2 = self.conv3(input)\n",
    "    conv2 = self.conv4(conv2)\n",
    "\n",
    "    concat = concatenate([conv1,conv2])\n",
    "    conv3 = self.conv5(concat)\n",
    "    add1 = Add()([input,conv3])\n",
    "\n",
    "    conv4 = self.conv6(add1)\n",
    "    conv4 = self.conv7(conv4)\n",
    "    add2 = Add()([conv4,add1])\n",
    "    add2 = Activation('relu')(add2)\n",
    "\n",
    "    conv5 = self.conv8(add2)\n",
    "    conv5 = self.conv9(conv5)\n",
    "    conv5 = self.conv10(conv5)\n",
    "    add3 = Add()([add2,conv5])\n",
    "    add3 = Activation('relu')(add3)\n",
    "\n",
    "    gap = self.gap(add3)\n",
    "    gap = Reshape((1,1,64))(gap)\n",
    "    conv6 = self.conv11(gap)\n",
    "    conv6 = self.conv12(conv6)\n",
    "    \n",
    "    mul = Multiply()([conv6, add3])\n",
    "    out = Add()([input,mul]) # This is not included in the reference code\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9682db8-39ef-43c2-957e-2e25be209a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input = Input(shape=(256, 256, 3))\n",
    "\n",
    "conv1 = Conv2D(64, (3,3),padding='same')(input)\n",
    "eam1 = EAM()(conv1)\n",
    "eam2 = EAM()(eam1)\n",
    "eam3 = EAM()(eam2)\n",
    "eam4 = EAM()(eam3)\n",
    "#add = Add()([eam4,conv1])  \n",
    "conv2 = Conv2D(3, (3,3),padding='same')(eam4)\n",
    "out = Add()([conv2,input])\n",
    "\n",
    "RIDNet = Model(input,out)\n",
    "RIDNet.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "RIDNet.summary()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1a879-6ef6-440e-8460-663b001bdd04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_dataloader = Dataloder(X_train_dataframe,y_train_dataframe, batch_size, shuffle=True)\n",
    "test_dataloader = Dataloder(X_test_dataframe,y_test_dataframe,batch_size, shuffle=True)\n",
    "validate_dataloader = Dataloder(X_validate_dataframe,y_validate_dataframe,batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d39022-8e0f-4759-8609-618fe0332141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir=\"logs/model_4\"\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,verbose=1,patience=4,min_delta=0.00001)\n",
    "callback = [tensorboard,reducelr]\n",
    "RIDNet.fit(train_dataloader,shuffle=True,epochs=20,validation_data= test_dataloader, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c020c-32db-4bce-8d09-bd39038380b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RIDNet.fit(train_dataloader,shuffle=True,epochs=25,initial_epoch=20,validation_data= test_dataloader, callbacks=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9d727-153b-47be-8507-498c1bc9878f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize(sample,RIDNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987c90b-8897-4559-a095-569723daf4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "psnr_nsy, psnr_de_nsy, ssim_nsy, ssim_de_nsy = psnr_and_ssim(test_df,RIDNet)\n",
    "print('PSNR before denoising :', psnr_nsy)\n",
    "print('PSNR after denoising :', psnr_de_nsy)\n",
    "print('SSIM before denoising :', ssim_nsy)\n",
    "print('SSIM after denoising :', ssim_de_nsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840af39-319b-408b-9181-5aaf3659c96c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RIDNet.save('RIDNet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708248e6-f5a3-4654-be14-78f29fbe455f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RIDNet = tf.keras.models.load_model('RIDNet.h5',custom_objects={'EAM':EAM})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdfdfd6-e731-4d76-a28b-54874595256e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_size = round((os.stat('RIDNet.h5').st_size)/(1024**2),3)\n",
    "PSNR.append(round(psnr_de_nsy,3))\n",
    "SSIM.append(round(ssim_de_nsy,3))\n",
    "PSNR_imp.append(round(psnr_de_nsy-psnr_nsy,3))\n",
    "SSIM_imp.append(round(ssim_de_nsy-ssim_nsy,3))\n",
    "size.append(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e673b4-c581-4430-8522-e2ab592a56ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def calculate_average_rmse(test_df, model, model_type='Normal'):\n",
    "    total_rmse = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for i in range(len(test_df)):\n",
    "        # Load the noisy image\n",
    "        nsy_path = test_df['shabby_image_path'].iloc[i]\n",
    "        nsy = cv2.imread(nsy_path)\n",
    "        if nsy is None:\n",
    "            print(f\"Error loading noisy image: {nsy_path}\")\n",
    "            continue  # Skip this iteration and move to the next\n",
    "\n",
    "        # Getting the predicted images\n",
    "        if model_type == 'Quantized': \n",
    "            pred = prediction_tflite(nsy, model)\n",
    "        else:\n",
    "            pred = prediction(nsy, model)\n",
    "\n",
    "        # Getting the ground truth image data\n",
    "        gt_path = test_df['cleaned_image_path'].iloc[i]\n",
    "        gt = cv2.imread(gt_path)\n",
    "        if gt is None:\n",
    "            print(f\"Error loading ground truth image: {gt_path}\")\n",
    "            continue  # Skip this iteration and move to the next\n",
    "        gt = cv2.cvtColor(gt, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize the predicted image to match the ground truth image dimensions\n",
    "        pred = cv2.resize(pred, (gt.shape[1], gt.shape[0]))\n",
    "\n",
    "        # Calculate RMSE for the current image\n",
    "        rmse = np.sqrt(np.mean(np.square(gt - pred)))\n",
    "\n",
    "        # Accumulate RMSE values\n",
    "        total_rmse += rmse\n",
    "        num_samples += 1\n",
    "\n",
    "    # Calculate the average RMSE over all test images\n",
    "    average_rmse = total_rmse / num_samples\n",
    "    return average_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bef83-ea11-444b-b6e5-26d370530042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the models (replace 'model1_path' and 'model2_path' with the actual file paths)\n",
    "model1 = tf.keras.models.load_model('autoencoder.h5')\n",
    "model2 = tf.keras.models.load_model('CBDNet.h5')\n",
    "# Load PRIDNet model with custom layer registration\n",
    "model3 = tf.keras.models.load_model('PRIDNet.h5', custom_objects={'convolutional_block1':convolutional_block1, 'CAM':CAM,'convolutional_block2':convolutional_block2,'pyramid':pyramid,'KSM':KSM})\n",
    "# Calculate average RMSE for each model\n",
    "average_rmse_model1 = calculate_average_rmse(test_df,model1)\n",
    "average_rmse_model2 = calculate_average_rmse(test_df,model2)\n",
    "average_rmse_model3 = calculate_average_rmse(test_df,model3)\n",
    "\n",
    "print(\"Average RMSE for Autoencoder (Baseline model):\", average_rmse_model1)\n",
    "print(\"Average RMSE for CBDNet Model:\", average_rmse_model2)\n",
    "print(\"Average RMSE for PRIDNet Model:\", average_rmse_model3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a2343-1e39-4003-927b-9fddd04f0a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
